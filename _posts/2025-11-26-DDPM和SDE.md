---
title: DDPM和SDE
date: 2025-11-26
categories:
  - Diffusion
tags:
  - DDPM
---
> **DDPM是diffusion SDE的一阶离散**

回想一下，我们在DDPM里的加噪过程。每一个time step，我们都会按照如下的离散马尔可夫链进行加噪：

$$x_i = \sqrt{1 - \beta_i} x_{i-1} + \sqrt{\beta_i} \epsilon_{i-1}$$

但是论文里面提到要将这个离散过程连续化变成SDE（随机微分方程）。
## 为什么我们要将离散的加噪过程连续化？

我们可以用**“走楼梯 vs. 滑滑梯”**来比喻。

- **离散 DDPM** 就像是修好的**楼梯**，规定了必须有 1000 级台阶。你只能一步一步走。
- **连续 SDE** 就像是一个平滑的**滑滑梯**（或者斜坡）。

训练与采样的“解耦” (Decoupling) —— 最大的实用价值

- 离散的痛点（DDPM）：
    在 DDPM 中，如果你训练时设定了 $N=1000$ 步，那么你在生成图片（推理/采样）时，通常也必须老老实实走完这 1000 步。如果你想快点，只走 50 步，效果通常会变差，或者需要重新训练一个 $N=50$ 的模型。
    - _训练和推理被绑死了。_

- 连续的优势（SDE）：
    一旦我们将模型建模为连续的 SDE，我们就学习到了这个物理过程的全貌（原本的函数曲线）。
    在生成图片时，我们可以自由决定怎么走：
    - 你可以用 **1000 步** 慢慢走，画质极度细腻。
    - 你可以用 **20 步** 大跨步走（离散化），快速出图。
    - **我们不需要重新训练模型**，只需要在采样时改变“步长”即可。这让扩散模型的实用性大大增强。

## **DDPM到SDE的推导**

### 首先引入一组辅助的noise scale
令 $\beta_i = \frac{\bar{\beta}_i}{N}$，我们保证了无论切分成多少步，$N \times \beta_i$（总体的噪声强度）大致保持不变，从而顺利地取极限得到连续函数 $\beta(t)$。
为什么引入这个noise scale？
我看了很多资料，似乎都跳过了这一点。
下面说说我自己的理解：
1. 首先是$\beta_i$ 是不断变小的，我们需要构造出$\Delta t$ 那么就必须除以一个N。于是就产生了这种令 $\beta_i = \frac{\bar{\beta}_i}{N}$
2. 以前一步变化的$\Delta\beta$是$\beta_i - \beta_{i-1}$ 现在我们的横坐标减少了N倍，从1-N变成了0-1，所以我们现在要求变化$1/N$ ，$\Delta\beta$不变，那么自然要把$\beta_i$扩大N倍。由此得出$\bar{\beta_i} = \beta_i N$
此处$\beta(t)$ 是加噪计划，$x(t)$ 是每次加噪之前充满噪点的图像, $\epsilon(t)$ 是噪声的强度
将上面的式子改写如下：

$$
x_i = \sqrt{1 - \frac{\bar{\beta}_i}{N}} x_{i-1} + \sqrt{\frac{\bar{\beta}_i}{N}} \epsilon_{i-1}, \quad i=1, \dots, N
$$

在 $N \to \infty$ 时，上面的 $\{\bar{\beta}_i\}_{i=1}^N$ 就成了一个关于时间 $t$ 的连续函数 $\beta(t)$，并且 $t \in [0, 1]$。随后，我们可以假设 $\Delta t = \frac{1}{N}$，在每个 $i\Delta t$ 时刻，连续函数 $\beta(t), x(t), \epsilon(t)$ 都等于之前的离散值，即：

$$
\beta\left(\frac{i}{N}\right) = \bar{\beta}_i, \quad x\left(\frac{i}{N}\right) = x_i, \quad \epsilon\left(\frac{i}{N}\right) = \epsilon_i
$$

### 接下来就是要往微分方程上靠
SDE 的标准长相是：

$$\mathrm{d}x = f(x,t)\mathrm{d}t + g(t)\mathrm{d}w$$

这里的 $\mathrm{d}x$ 意思是：在极短的时间内，x 变化了多少？
如果你想得到一个**微分方程**（即 $\mathrm{d}x = \dots$ 的形式），你必须先计算“下一步在哪里”，然后减去“现在在哪里”
也就是计算$x(t + \Delta t)$ 是为了凑出 **“变化量” ($\mathrm{d}x$)**
我们的最终目标是推导出 SDE（随机微分方程）。
之前离散状况下，i是在1-N。 现在连续了，改成t属于0-1了，
在 $t \in \{0, 1, \dots, \frac{N-1}{N}\}$ 以及 $\Delta t = \frac{1}{N}$ 的情况下，我们就可以用连续函数改写之前的式子： $$ \begin{aligned} x(t + \Delta t) &= \sqrt{1 - \beta(t + \Delta t)\Delta t} \, x(t) + \sqrt{\beta(t + \Delta t)\Delta t} \, \epsilon(t) \\ &\approx x(t) - \frac{1}{2}\beta(t + \Delta t)\Delta t \, x(t) + \sqrt{\beta(t + \Delta t)\Delta t} \, \epsilon(t) \\ &\approx x(t) - \frac{1}{2}\beta(t)\Delta t \, x(t) + \sqrt{\beta(t)\Delta t} \, \epsilon(t) \end{aligned} $$

---
### 第二行：线性化（泰勒展开大法）

$$\approx x(t) - \frac{1}{2}\beta(t + \Delta t)\Delta t \, x(t) + \sqrt{\beta(t + \Delta t)\Delta t} \, \epsilon(t)$$

- 发生了什么？
    这里对第一项系数 $\sqrt{1 - \dots}$ 动了手术。
- 数学原理（泰勒级数）：
    当 $x$ 非常非常小的时候，我们有一个黄金近似公式：$$\sqrt{1 - x} \approx 1 - \frac{1}{2}x$$(你可以试一下：$\sqrt{0.98} = \sqrt{1-0.02} \approx 1 - 0.01 = 0.99$。计算器按出来是 0.9899... 非常接近)
- 应用在这里：
    我们将 $x = \beta(t+\Delta t)\Delta t$ 代入：
    $$\sqrt{1 - \beta(t+\Delta t)\Delta t} \approx 1 - \frac{1}{2}\beta(t+\Delta t)\Delta t$$
    
- 结果：
    原来的乘法 $x(t)$ 变成了两部分：
    1. $1 \cdot x(t) = x(t)$ （保留原样）
    2. $- \frac{1}{2}\dots x(t)$ （衰减的一小部分）
- **注意**：第二项（噪声项）带根号 $\sqrt{\beta \Delta t}$，这里先不动它。因为 $\sqrt{\Delta t}$ 是 $0.5$ 次方，不能用这个泰勒公式。
---

### 第三行：平滑化（忽略高阶微小量）

$$\approx x(t) - \frac{1}{2}\beta(t)\Delta t \, x(t) + \sqrt{\beta(t)\Delta t} \, \epsilon(t)$$

- 发生了什么？
    把所有的 $t + \Delta t$ 直接写成了 $t$。
- 为什么可以这么做？
    因为 $\beta(t)$ 是我们设计的一个连续光滑的函数（比如线性的，或者余弦的）。
    当时间间隔 $\Delta t$ 趋近于 0 时，“下一瞬间的 $\beta$” 和 “现在的 $\beta$” 几乎没区别。$$\beta(t + \Delta t) \approx \beta(t)$$
- 目的：
    把式子里的变量统一，全部变成 $t$ 时刻的变量，方便后续处理。



上面的近似只有在 $\Delta t \ll 1$ 时成立。我们将其再移项后就可以得到下式： $$ \begin{aligned} x(t + \Delta t) - x(t) &\approx -\frac{1}{2}\beta(t)\Delta t \, x(t) + \sqrt{\beta(t)\Delta t} \, \epsilon(t) \\ \mathrm{d}x &= -\frac{1}{2}\beta(t)x \mathrm{d}t + \sqrt{\beta(t)} \mathrm{d}w \end{aligned} $$

tips：
1.  $\Delta t$ 是时间微元，当取极限时，直接写成微分形式 $\mathrm{d}t$。
2. $\mathrm{d}w$ 是怎么来的？（最关键的一步）
	这里的核心在于：**$\sqrt{\Delta t} \cdot \epsilon(t)$ 本质上就是维纳过程的增量 $\mathrm{d}w$。**
	**为什么？请看统计性质的匹配：**

	- 左边：$\sqrt{\Delta t} \cdot \epsilon(t)$**
    - $\epsilon(t)$ 是标准正态分布 $N(0, 1)$。
    - 乘以常数 $\sqrt{\Delta t}$ 后，根据方差性质 $\text{Var}(kX) = k^2\text{Var}(X)$：
    - 均值：$0$
    - 方差：$(\sqrt{\Delta t})^2 \cdot 1 = \Delta t$
    - **结论**：这是一个服从 $N(0, \Delta t)$ 的随机变量。

	- **右边：$\mathrm{d}w$ (或 $\Delta w$)**
    - 根据布朗运动（维纳过程）的定义：在时间 $\Delta t$ 内的增量 $w(t+\Delta t) - w(t)$ 服从正态分布。
    - 均值：$0$
    - 方差：$\Delta t$ （记得那个 $\text{Var} \propto t$ 的性质吗？）
    - **结论**：这也是一个服从 $N(0, \Delta t)$ 的随机变量。

**因为两者在数学分布上完全等价（均值为 0，方差都等于时间间隔 $\Delta t$），所以我们在写成微分方程时，直接用标准的随机符号 $\mathrm{d}w$ 来替换 $\sqrt{\Delta t} \epsilon(t)$。**


至此，我们证明了DDPM连续化之后，就可以得到一个SDE方程，并且回想一下上一节SDE里面的内容，我们发现这个SDE它是一种Variance Preserving的SDE。**Variance Preserving的含义是当**$t \to \infty$ **​时，它的方差依然有界**

既然前向过程连续化之后就是一个VP SDE，那么反向过程又是什么呢？答案就是，**反向过程也是一个SDE方程，称为reverse SDE**：


- [ ] DPPM反向过程推导
- [ ] DDIM和ODE
- [ ] https://zhuanlan.zhihu.com/p/610505558
- [ ] 李宏毅？
## DDPM反向过程的推导：
先放结论，反向过程的公式是:
$\mathrm{d}\mathbf{x} = \left[ \mathbf{f}(\mathbf{x}, t) - g^2(t) \nabla_\mathbf{x} \log p_t(\mathbf{x}) \right] \mathrm{d}t + g(t) \mathrm{d}\bar{\mathbf{w}}$
并且，这个反向过程中的未知量就只有分数函数$\nabla_\mathbf{x} \log p_t(\mathbf{x})$ ​。DDPM本质上都是在学习这个reverse SDE的解。


## **DDIM和ODE**


扩散模型本质上是在学习一个扩散过程的逆过程，既然前向SDE存在一个对应的ODE，**那么反向过程reverse SDE其实也有一个对应的ODE，这个反向过程对应的ODE形式也是上面的式子**。
既然引入了ODE，那么我们的模型就可以去学习如何解这个ODE，同时也可以引入各种传统的ODE solver例如：Euler method, Runge–Kutta method等一些方法。这就是为什么我们可以看到像Stable Diffusion之类的模型会有那么多sampler的原因，本质上都是一些ODE solver和SDE solver。

参考文献：
[^1]: Diffusion Model与SDE、ODE之间的联系 - Victor的文章 - 知乎 https://zhuanlan.zhihu.com/p/698737562